# AI Knowledge Base Ingest Pipeline

A robust document ingestion pipeline that syncs documents from Google Drive, processes them with Unstructured.io, stores them in S3, and indexes them in OpenAI Vector Stores for semantic search.

## ğŸ¯ Purpose

This pipeline is designed to **ingest and prepare documents** for AI-powered knowledge retrieval systems. It handles the complete workflow from Google Drive to OpenAI Vector Stores, making your documents searchable via semantic AI.

**Note**: This repository handles **ingestion only**. For querying the vector store and building Custom GPT APIs, see the **[api-reference/](api-reference/)** folder.

## ï¿½ Repository Organization

This repository is organized into **two clear sections**:

### ğŸ”§ Root Directory (Ingestion Pipeline)
**Purpose**: Sync documents from Google Drive to OpenAI Vector Stores

**Key files**:
- `main.py` - Pipeline entry point
- `src/` - Pipeline source code (drive_sync, processor, indexer)
- `data/pipeline.db` - SQLite database tracking file state

**Use this for**: Setting up and running the document ingestion workflow

### ğŸš€ api-reference/ Folder (Query API Reference)
**Purpose**: Sample implementation for querying the vector store (for your future Custom GPT API)

**Key files**:
- `direct_search.py` - Search implementation (NO LLM required)
- `README.md` - Complete API integration guide
- `QUICK_START.md` - 5-minute setup guide
- Performance benchmarks and accuracy reports

**Use this for**: Building your Custom GPT API, implementing semantic search

### ğŸ“š Quick Links

- **[api-reference/QUICK_START.md](api-reference/QUICK_START.md)** - 5-minute API setup
- **[api-reference/README.md](api-reference/README.md)** - Complete API guide

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Google Drive   â”‚
â”‚  (Source docs)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. SYNC (drive_sync.py)                â”‚
â”‚     - List changed files                â”‚
â”‚     - Download from Drive               â”‚
â”‚     - Upload to S3                      â”‚
â”‚     - Store metadata in SQLite          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. PROCESS (processor.py)              â”‚
â”‚     - Fetch from S3                     â”‚
â”‚     - Process with Unstructured.io      â”‚
â”‚     - Extract text, tables, images      â”‚
â”‚     - Save structured data to S3        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. INDEX (indexer.py)                  â”‚
â”‚     - Upload to OpenAI                  â”‚
â”‚     - Add to Vector Store               â”‚
â”‚     - Enable semantic search            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenAI Vector  â”‚
â”‚     Store       â”‚
â”‚  (Searchable)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Google Cloud service account with Drive API access
- AWS S3 bucket
- OpenAI API key with Vector Store access
- Unstructured.io API key (optional, for cloud processing)

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd ai-knowledge-base-ingest

# Install dependencies with uv (recommended)
uv sync

# Or with pip
pip install -e .
```

### Configuration

1. Copy the example environment file:
```bash
cp .env.example .env
```

2. Edit `.env` with your credentials:
```env
# Google Drive
GOOGLE_CREDENTIALS_FILE=path/to/service-account.json
DRIVE_FOLDER_ID=your-drive-folder-id

# AWS S3
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1
S3_BUCKET_NAME=your-bucket-name

# OpenAI
OPENAI_API_KEY=sk-proj-...
VECTOR_STORE_ID=vs_...

# Unstructured.io (optional)
UNSTRUCTURED_API_KEY=your-api-key
UNSTRUCTURED_API_URL=https://api.unstructuredapp.io/general/v0/general
```

## ğŸ“– Usage

### Basic Commands

```bash
# Full pipeline - sync, process, and index
uv run python main.py

# Individual stages
uv run python main.py sync              # Sync from Drive to S3
uv run python main.py process           # Process files with Unstructured
uv run python main.py index             # Index files to OpenAI

# Dry run (test without changes)
uv run python main.py --dry-run sync

# Limit number of files
uv run python main.py --max-files 10 sync
```

### Pipeline Stages

#### 1. Sync Stage
Syncs files from Google Drive to S3:
- Lists all files in the configured Drive folder (recursive)
- Detects changes using modification timestamps and SHA256 hashes
- Downloads changed files from Drive
- Uploads to S3 with metadata
- Exports Google Workspace files (Docs, Sheets, Slides) to Office formats
- Stores file state in SQLite database

```bash
uv run python main.py sync --max-files 100
```

#### 2. Process Stage
Processes files with Unstructured.io:
- Fetches raw files from S3
- Extracts text, tables, and images
- Structures content into chunks
- Saves processed output back to S3
- Supports 30+ file formats (PDF, DOCX, PPTX, images, etc.)

```bash
uv run python main.py process --max-files 50
```

#### 3. Index Stage
Indexes processed files to OpenAI Vector Store:
- Uploads processed text files to OpenAI
- Adds files to Vector Store
- Enables semantic search
- Updates database with OpenAI file IDs

```bash
uv run python main.py index --max-files 50
```

## ğŸ—„ï¸ Database Schema

The pipeline uses SQLite to track file state:

```sql
-- Main file tracking table
CREATE TABLE file_state (
    id INTEGER PRIMARY KEY,
    sha256 TEXT UNIQUE NOT NULL,
    s3_key TEXT NOT NULL,
    original_name TEXT NOT NULL,
    mime_type TEXT,
    size_bytes INTEGER,
    status TEXT NOT NULL,
    synced_at TIMESTAMP,
    processed_at TIMESTAMP,
    indexed_at TIMESTAMP,
    openai_file_id TEXT,
    error_message TEXT
);

-- Google Drive file mapping
CREATE TABLE drive_file_mapping (
    id INTEGER PRIMARY KEY,
    sha256 TEXT NOT NULL,
    drive_file_id TEXT NOT NULL,
    drive_file_name TEXT NOT NULL,
    drive_parent_path TEXT,
    drive_modified_time TIMESTAMP,
    FOREIGN KEY (sha256) REFERENCES file_state(sha256)
);
```

## ğŸ“ Project Structure

```
ai-knowledge-base-ingest/
â”œâ”€â”€ main.py                  # Main entry point
â”œâ”€â”€ purge.py                 # Cleanup utility
â”œâ”€â”€ pyproject.toml           # Dependencies
â”œâ”€â”€ docker-compose.yml       # Docker setup
â”œâ”€â”€ Dockerfile               # Container definition
â”œâ”€â”€ .env                     # Configuration (not in git)
â”œâ”€â”€ .env.example             # Configuration template
â”œâ”€â”€ data/                    # Database and temp files
â”‚   â””â”€â”€ pipeline.db          # SQLite database
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ config.py            # Configuration management
â”‚   â”œâ”€â”€ database.py          # Database operations
â”‚   â”œâ”€â”€ drive_sync.py        # Google Drive sync
â”‚   â”œâ”€â”€ processor.py         # Unstructured.io processing
â”‚   â”œâ”€â”€ indexer.py           # OpenAI indexing
â”‚   â””â”€â”€ utils.py             # Shared utilities
â””â”€â”€ api-reference/           # API reference for querying
    â”œâ”€â”€ README.md            # API documentation
    â”œâ”€â”€ direct_search.py     # Vector store search sample
    â””â”€â”€ *.md                 # Analysis and benchmarks
```

## ğŸ”§ Advanced Usage

### Dry Run Mode

Test the pipeline without making changes:
```bash
uv run python main.py --dry-run sync
uv run python main.py --dry-run process
```

### Limit Files

Process only a subset of files:
```bash
uv run python main.py --max-files 5 sync
uv run python main.py --max-files 10 process
```

### Verbose Logging

Enable detailed debug output:
```bash
uv run python main.py --verbose sync
```

### Purge Data

Clean up all data (âš ï¸ destructive):
```bash
uv run python purge.py
```

Options:
- `--all` - Delete everything (database, S3, OpenAI)
- `--s3` - Delete only S3 objects
- `--openai` - Delete only OpenAI files
- `--db` - Delete only database

## ğŸ³ Docker Deployment

### Using Docker Compose

```bash
# Build and run
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

### Using Docker Directly

```bash
# Build image
docker build -t knowledge-ingest .

# Run container
docker run -d \
  --name knowledge-ingest \
  --env-file .env \
  -v $(pwd)/data:/app/data \
  knowledge-ingest
```

## ğŸ“Š Monitoring

The pipeline provides detailed progress tracking:

```
Syncing from Google Drive...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00
âœ“ Successfully synced 10 files

Processing with Unstructured.io...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:30
âœ“ Successfully processed 10 files

Indexing to OpenAI Vector Store...
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:01:00
âœ“ Successfully indexed 10 files
```

Check database for file status:
```bash
sqlite3 data/pipeline.db "SELECT status, COUNT(*) FROM file_state GROUP BY status"
```

## ğŸ” Supported File Types

The pipeline supports 30+ file formats:

**Documents**: PDF, DOCX, DOC, ODT, RTF, TXT, MD  
**Spreadsheets**: XLSX, XLS, CSV, ODS  
**Presentations**: PPTX, PPT, ODP  
**Images**: JPG, PNG, TIFF (with OCR)  
**Google Workspace**: Docs, Sheets, Slides (auto-converted)  
**Archives**: ZIP (auto-extracted)  
**Email**: EML, MSG  

## ğŸš¦ File Processing States

Files move through these states:

1. **synced** - Downloaded from Drive, uploaded to S3
2. **processed** - Processed by Unstructured.io
3. **indexed** - Indexed in OpenAI Vector Store
4. **error** - Processing failed (check error_message)

## ğŸ”— Integration with Custom GPT API

After ingestion, documents are searchable via OpenAI Vector Stores. See the `api-reference/` folder for:

- **direct_search.py** - Sample implementation for semantic search
- **README.md** - Complete API integration guide
- Performance benchmarks and test results
- Code samples for FastAPI, Flask, Express

The API reference shows how to:
- Query the vector store without using an LLM (5x faster, cheaper)
- Enrich results with Drive URLs and S3 presigned URLs
- Build Custom GPT actions
- Handle multilingual content

## ğŸ› Troubleshooting

### Common Issues

**Google Drive authentication fails**
```bash
# Check service account file exists
ls -la your-service-account.json

# Verify Drive API is enabled
# Check permissions in Google Cloud Console
```

**S3 upload fails**
```bash
# Test AWS credentials
aws s3 ls s3://your-bucket-name/

# Check IAM permissions (s3:PutObject, s3:GetObject)
```

**OpenAI indexing fails**
```bash
# Verify API key
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
  https://api.openai.com/v1/models

# Check vector store exists
# Verify API key has vector store access
```

**Processing fails**
```bash
# Check Unstructured.io API key
curl -H "unstructured-api-key: $UNSTRUCTURED_API_KEY" \
  https://api.unstructuredapp.io/general/v0/general

# Try processing locally (no API key needed)
# Set UNSTRUCTURED_API_KEY="" in .env
```

### Debug Mode

Enable detailed logging:
```bash
uv run python main.py --verbose sync 2>&1 | tee debug.log
```

Check database state:
```bash
sqlite3 data/pipeline.db

# List all tables
.tables

# Check file states
SELECT status, COUNT(*) FROM file_state GROUP BY status;

# View recent errors
SELECT original_name, error_message, synced_at 
FROM file_state 
WHERE status = 'error' 
ORDER BY synced_at DESC 
LIMIT 10;
```

## ğŸ“ˆ Performance

Typical processing times:
- **Sync**: ~1-2 seconds per file (depends on Drive API)
- **Process**: ~5-10 seconds per file (depends on Unstructured.io)
- **Index**: ~2-3 seconds per file (depends on OpenAI API)

Batch recommendations:
- Start with `--max-files 10` to test
- Scale up to `--max-files 100` for production
- Use `--dry-run` to preview changes

## ğŸ”’ Security

- Keep `.env` file secure (never commit to git)
- Use least-privilege IAM policies for S3
- Rotate API keys regularly
- Use service accounts for Google Drive (not personal accounts)
- Limit Drive folder access to only necessary files
- Enable S3 bucket encryption
- Use presigned URLs with short expiration (1 hour)

## ğŸ¤ Contributing

This is an internal tool, but improvements are welcome:

1. Test changes thoroughly with `--dry-run`
2. Update documentation
3. Check error handling
4. Verify database migrations

## ğŸ“ License

Internal use only.

## ğŸ†˜ Support

For issues with:
- **Ingestion pipeline**: Check this README and troubleshooting section
- **API integration**: See `api-reference/README.md`
- **Custom GPT setup**: Refer to API reference documentation

---

## ğŸ”„ Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: INGEST (Root Directory)                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Run: uv run python main.py                                 â”‚
â”‚                                                              â”‚
â”‚  1. Sync from Google Drive to S3                            â”‚
â”‚  2. Process with Unstructured.io                            â”‚
â”‚  3. Index to OpenAI Vector Store                            â”‚
â”‚                                                              â”‚
â”‚  Result: Documents searchable in vector store               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: BUILD API (api-reference/)                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Reference: api-reference/README.md                         â”‚
â”‚  Sample: api-reference/direct_search.py                     â”‚
â”‚                                                              â”‚
â”‚  1. Copy direct_search.py to your API project              â”‚
â”‚  2. Implement HTTP endpoint (FastAPI/Flask/Express)        â”‚
â”‚  3. Deploy your API                                         â”‚
â”‚  4. Connect Custom GPT                                      â”‚
â”‚                                                              â”‚
â”‚  Result: Custom GPT can query your knowledge base          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ingestion**: Keeps vector store up-to-date with latest documents (run periodically)  
**API**: Queries vector store on-demand for Custom GPT (always-on service)

---

## ğŸ“¦ Version Control

### What to Commit

âœ… **Safe to commit**:
- All source code (`main.py`, `src/`, `api-reference/`)
- Documentation files (`.md`)
- Configuration templates (`.env.example`)
- Docker files (`Dockerfile`, `docker-compose.yml`)
- Dependencies (`pyproject.toml`, `uv.lock`)

âš ï¸ **Check .gitignore before committing**:
- `.env` (should be ignored - contains secrets)
- `data/pipeline.db` (may contain sensitive file metadata)
- `*.json` (service account credentials)

âŒ **Never commit**:
- `.env` file with credentials
- Service account JSON files
- Database with production data
- API keys or secrets

### Suggested Commit Message

```bash
git add -A
git commit -m "refactor: Separate ingestion pipeline from API reference

Reorganized repository for clear separation of concerns:

ADDED:
- Comprehensive documentation for ingestion and API
- api-reference/ folder with complete integration guide
- Enhanced direct_search.py with --full and --preview-length options

REMOVED FROM ROOT:
- Query tools and documentation (moved to api-reference/)

BENEFITS:
- Clear separation: ingestion vs API development
- Better documentation structure
- Ready for Custom GPT API integration"

git push origin main
```

---

## ğŸ“ Next Steps

### For Ingestion Setup
1. âœ… Read this README
2. âœ… Configure `.env` with your credentials
3. âœ… Run `uv run python main.py --dry-run sync` to test
4. âœ… Run full pipeline: `uv run python main.py`

### For API Development
1. âœ… Go to `api-reference/` folder
2. âœ… Read `QUICK_START.md` (5 minutes)
3. âœ… Test: `uv run python api-reference/direct_search.py "test query"`
4. âœ… Review code samples in `README.md`
5. âœ… Copy to your API project and deploy

---

**Ready to start?**
1. Configure `.env` with your credentials
2. Run `uv run python main.py --dry-run sync` to test
3. Process files with `uv run python main.py`
4. Build your API using `api-reference/` as a guide
